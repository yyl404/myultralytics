## baseline training
第一个实验，是在voc数据集上进行了yolo11m baseline的训练，用于和后续增量学习情景进行对照。实验结果保存至results_baseline.zip。

## 2025/08/09

跑了三组实验：naive, pseudo labels和dual teachers。naive就是用同一组权重，顺序地在每个任务集上进行训练，不采取任何增量学习的手段。
pseudo labels就是在学习每个任务集之前，用上一个任务学习得到的权重在新数据集上进行推理，将推理的结果保存成伪标签，添加到数据集标签当中，起到模型蒸馏的作用。
dual teachers是在新数据集上训练一个专家模型，然后分别用专家模型和上一个数据集训练得到的基础模型进行推理，生成伪标签，相当于同时蒸馏专家模型和基础模型。
在这一次的实验里，伪标签仅仅在训练集上生成，未在验证集上生成。这是为了避免旧任务的类别伪标签当中，检测错误的实例干扰模型评估，使得选取到次优的模型权重。但是从实验结果来看，验证集上不使用伪标签，会导致模型的遗忘较为明显。

实验结果保存至results_20250809.zip。

## 2025/08/11

跑了两组实验，yolov11m_inc_15_1_1_1_1_1_full-labels_fromscratch_naive和yolov11m_inc_15_1_1_1_1_1_full-labels_fromscratch_naive_2。

这两组实验使用的是VOC_inc_15_1_1_1_1_1_full-labels数据集，这个数据集是基于VOC_inc_15_1_1_1_1_1构建的，但是在每一个增量任务训练集当中，不仅包含当前任务类别实例的标注，还包括此前所学习的全部类别实例的标注。这样是为了实验出伪标签蒸馏方法理论上的最优效果：即教师模型能够以100%准确率识别出任务数据集当中含有的历史类别实例。

其中yolov11m_inc_15_1_1_1_1_1_full-labels_fromscratch_naive的验证集既包含旧任务类别的实例标签，又包含新任务类别的实例标签。而yolov11m_inc_15_1_1_1_1_1_full-labels_fromscratch_naive_2的验证集只包含新任务类别的实例标签。

实验结果显示，单独的伪标签蒸馏手段，无法实现对抗遗忘的效果。这是由于新任务数据集当中，新类别实例远远超过旧类别实例，造成类别监督信号不平衡。并且还观察到，任务数据集的规模较小，丰富程度较低的时候，会出现过拟合现象。

实验结果保存至results_20250811.zip。

# 2025/08/12

将基础模型替换为yolov8m，重新实验了伪标签、双教师、全标签对照组。结果与yolo11m区别不大。

# 2025/08/19

进行了一组实验，使用了数据集VOC_inc_15_1_1_1_1_1_full-labels_only-train，用来模拟伪标签策略的理论最优情况。训练过程中每3个epoch保存一次checkpoint，并绘制了在不同任务数据集上训练时，模型在各个任务的验证集上精度变化曲线，用来评估遗忘速度。我发现模型在旧任务的精度并不是预想当中持续的下降，而是在开头若干个epoch之内迅速下降到某个稳定的值，然后不再变化。相关结果保存至results_20250819.zip。

# 2025/09/09

跑了一组实验，按照VOC数据集上的15+5类增量进行实验。其中增量学习阶段，只更新检测头的权重。结果保存至results-20250909。测试结果如下。

||1-15|16-20|
|-|-|-|
|model-1|75.2|-|
|model-2|70.8|55.3|

从结果上来看，虽然有效缓解了遗忘现象，但是同时也极大地损害了增量学习阶段的学习效果。

# 2025/09/10

有两组实验结果。首先是增加了特征层蒸馏损失函数的实验结果，如下所示

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|54|75.2|

重新实验了一遍只用伪标签进行蒸馏，并且所有层均可微调权重的实验结果，如下所示：

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|52.2|74.7|

看起来似乎使用特征层蒸馏有些许提升，但是难以排除是不是随机波动的影响，而且提升效果没法达到实用程度。

# 2025/09/11

继续实验在第二阶段冻结部分权重的思路，首先是冻结整个backbone（1-9层），实验结果如下

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|60.1|69.6|

对于遗忘的减轻十分显著，同时也没有过多地损害第二阶段本身的训练精度。

又在此基础上实现了OSR方法，结果如下

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|57.1|69.8|

似乎使用了OSR方法以后遗忘反而加深了。这可能是由于OSR方法的样本回放，会增加数据集规模，所以在相同的训练轮数下，迭代次数增大了。而OSR所生成的回放样本与真实的历史样本分布有偏差，所以进行了更多次数的迭代以后，反而使得模型的分布偏移更大了。

最后是，实现了对于每一个卷积模块输入特征图的PCA，并且绘制了可视化。从可视化中可以观察到，特征图的通道确实存在冗余，真正有效的特征方向是比较稀疏的。而且从图像上观察会发现，经过对数变换的特征值，与通道序号之间似乎存在双曲正切或者正切的函数关系，这一关系值得进一步研究。

# 2025/09/12

试验了投影损失函数，得到结果如下

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|69.6|66.2|

与之相对比的是，不使用投影损失函数，仅仅使用伪标签蒸馏的方法，得到的结果如下
||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|60.1|69.6|

从结果上来看，使用了投影损失函数的方法，的确对于缓解遗忘和增加新任务的精度有更好的平衡性。

# 2025/09/13

重新测试了投影损失函数，得到结果如下

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|68.8|66.9|

依然说明了该方法的有效性。

另外，我取消了对backbone的冻结，得到结果如下

||1-15|16-20|
|-|-|-|
|model-1|75.1|-|
|model-2|67.5|69.6|

和冻结backbone得到的结果差异不大。

# 2025/09/17

今天计划要做消融实验。

1. 利用VSPReg约束后的更新权重，它和显著子空间的夹角有多大？
我们首先应该给显著子空间划定一个标准，假定按照分位数p来取值，方差从大到小前p个方向所张成的空间称之为显著子空间。p需要根据可视化以后的方差分布图进行主观选取。
划定了显著子空间标准以后，我们要计算权重更新值在子空间上的投影长度，然后根据投影长度和更新向量长度的比值关系，计算出余弦值和夹角大小。
在可视化层面上，我们要绘制出每一层的若干个卷积核，其更新值与显著子空间夹角大小和余弦值的分布图，绘制成谱线图、频数直方图和排序后的数值分布图。

2. VSPReg能否使模型随着训练的加深，特征分布趋于方向稠密
划定多个训练阶段，每一个阶段结束以后，都在前几个阶段的数据集上进行逐层PCA并保存结果，可视化每一次PCA的结果。理想状况下，随着训练阶段的增多，各个主成分方向的方差应该趋于平衡，方向的利用情况应该趋于稠密。也就是按照方差从大到小排序后的排序图里，方差下降的曲线应该放缓。

3. VSPReg会不会影响权重更新？
对比使用VSPReg和不使用情况下，模型训练后的权重更新长度。这样可以看出VSPReg是仅仅约束了权重更新的方向，还是会限制权重更新的长度。